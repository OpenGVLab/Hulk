from functools import partial
from typing import Dict, Iterable, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
from dict_recursive_update import recursive_update
import numpy as np
from .recons_loss import recons_loss_entry
import os


class TextProjector(nn.Module):
    modality = 'text'
    """
    Project the decoder output to the text embedding space. A simple implementation that is now only work for attribute
    recognition tasks. The text embedding is generated by the BERT model. The text embedding is the average of the
    tokenized attributed word ids.
    :param: loss_cfg (di): the config of the loss function
    :param: hidden_dim: the hidden dimension of the decoder output
    :param: post_mul_norm: whether to normalize the output of the matrix multiplication of the decoder output and the
            text embedding
    :param: close_set: whether to use the close set setting. If True, the text embedding only contains few classes.
    :param: one_way_semantics: whether to use the one-way semantics setting. If True, the text embedding is only generated
                with the positive term of the attribute.
    :param: patch_size: A placeholder to achieve similar kwargs with other projector
    :param: in_chans: A placeholder to achieve similar kwargs with other projector
    :param: stride_level: A placeholder to achieve similar kwargs with other projector
    :param: description_dict_name: the name of the description dict. Default: 'rap2_attr_name'
    :param: task_sp_list: List of task specific list for DDP communication. Default: ()
    :param: ginfo: the global info for the DDP communication. Default: None
    """
    def __init__(self,
                 loss_cfg,
                 hidden_dim=256,
                 post_mul_norm=False,
                 one_way_semantics=False,
                 patch_size=[None, None],
                 in_chans=None,
                 stride_level=None,
                 pre_proj_type='',
                 description_dict_name='rap2_attr_name',
                 skeleton_action=False,
                 task_sp_list=(),
                 modality_share_list=(),
                 ginfo=None,
                 pre_extracted=False,
                 skeleton_action_one_hot_label=False,
                 replace_post_mul_norm: bool = False,
                 translate_weight_scale: float = 5.0,
                 post_mul_norm_scale: float = 1.0,
                 people_cnt=2,
                 image_caption=False,
                 vocal_size=30522, # for image caption embedding layer init
                 text_dict = None
                 ):
        super(TextProjector, self).__init__()
        # # placeholder, achieve similar kwargs with other projector
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.stride_level = stride_level
        self.pre_proj_type = pre_proj_type

        self.one_way_semantics = one_way_semantics
        self.skeleton_action = skeleton_action
        self.skeleton_action_one_hot_label = skeleton_action_one_hot_label
        self.replace_post_mul_norm = replace_post_mul_norm
        self.people_cnt = people_cnt
        self.image_caption = image_caption

        # for close set, the text embedding is the average of the tokenized attributed word ids using the BERT model
        if isinstance(description_dict_name, list):
            text_vectors = []
            for this_dict_name in description_dict_name:
                text_vectors.append(torch.load(f'./{this_dict_name}.pth'))
            text_vectors = torch.cat(text_vectors, dim=0)
        else:
            text_vectors = torch.load(f'./{description_dict_name}.pth')

        if self.one_way_semantics and not self.image_caption:
            text_vectors = text_vectors[:, 1]  # select the appearence features
        self.text_vectors = nn.Parameter(torch.zeros_like(text_vectors), requires_grad=False)
        self.num_tokens = self.text_vectors.shape[0]
        self.text_vectors.data.copy_(text_vectors)

        text_vectors_dim = text_vectors.shape[-1]
        # import pdb;pdb.set_trace()
        self.patch_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)

        self.class_proj = nn.Linear(text_vectors_dim, hidden_dim, bias=False)

        if replace_post_mul_norm:
            self.translate_weight = nn.Parameter(torch.ones([1]) * translate_weight_scale)
            self.translate_bias = nn.Parameter(torch.zeros([1]))
        elif post_mul_norm:
            self.post_mul_norm = nn.LayerNorm(self.num_tokens, eps=1e-6)
            nn.init.constant_(self.post_mul_norm.bias, 0)
            nn.init.constant_(self.post_mul_norm.weight, post_mul_norm_scale)
        else:
            self.post_mul_norm = None

        self.loss_cfg = loss_cfg
        self.loss_fn = recons_loss_entry(loss_cfg)
        self.task_sp_list = task_sp_list
        self.modality_share_list = modality_share_list
        self.ginfo = ginfo


    def forward(self, x, mask=None):
        """
        :param x: input dict that contains: "backbone_masking_info", "decoder_output", "label", etc.
        :param mask:
        :return: loss_dict(train)/ output(eval). loss_dict contains the loss of the text projector. output contains the
                predicted logits of the attribute(semantic existence).
        """
        task_mask = x.backbone_masking_info.task_masks[self.modality]
        task_infos = x.backbone_masking_info.task_infos
        text_output = x.decoder_output[:, task_infos['tasks'][self.modality]['start_idx']:task_infos['tasks'][self.modality]['end_idx']]

        B, _, C = text_output.shape

        # import pdb;pdb.set_trace()
        if self.pre_proj_type=='':
            #  for 2d attribute

            cls_text_feat = self.class_proj(self.text_vectors)
            patches = self.patch_proj(text_output)
            patches = F.normalize(patches, dim=2, p=2)  # [bs, num_classes, dim]

            if not self.one_way_semantics:
                cls_text_feat = F.normalize(cls_text_feat, dim=2, p=2)  # [num_classes, 3, dim] if not self.one_way_semantics
                # masks = patches @ cls_seg_feat.transpose(1, 2)
                sim_matrix = torch.einsum('bcd,cld->bcl', patches, cls_text_feat)  # [bs, num_classes, 3]
                if self.replace_post_mul_norm:
                    sim_matrix = sim_matrix * self.translate_weight + self.translate_bias
                elif self.post_mul_norm is not None:
                    sim_matrix = self.post_mul_norm(sim_matrix.permute(0, 2, 1)).permute(0, 2, 1)
            else:
                cls_text_feat = F.normalize(cls_text_feat, dim=1, p=2)
                if self.image_caption:
                    sim_matrix = torch.einsum('bcd, nd->bcn', patches, cls_text_feat)
                else:
                    sim_matrix = torch.einsum('bcd, cd->bc', patches, cls_text_feat)
                if self.replace_post_mul_norm:
                    sim_matrix = sim_matrix * self.translate_weight + self.translate_bias
                elif self.post_mul_norm is not None:
                    sim_matrix = self.post_mul_norm(sim_matrix)
        elif self.pre_proj_type=='pool':
            #  for skeleton action
            cls_text_feat = self.class_proj(self.text_vectors)

            cls_text_feat = F.normalize(cls_text_feat, dim=1, p=2)
            if self.skeleton_action:
                # default M=2, batch_size = B//2,
                if self.skeleton_action_one_hot_label:
                    text_output = text_output.reshape(B // self.people_cnt, self.people_cnt, -1, text_output.shape[-1])
                    patches = text_output.mean(dim=1)
                    patches = self.patch_proj(patches)
                    patches = F.normalize(patches, dim=2, p=2)  # [bs, cls, dim]
                else:
                    text_output = text_output.reshape(B//2, 2, -1)

                    patches = text_output.mean(dim=1)
                    patches = self.patch_proj(patches)
                    patches = F.normalize(patches, dim=1, p=2)  # [bs, dim]

                if self.skeleton_action_one_hot_label:
                    sim_matrix = torch.einsum('bcd, cd->bc', patches, cls_text_feat)
                else:
                    sim_matrix = torch.einsum('bd, cd->bc', patches, cls_text_feat)
                if self.replace_post_mul_norm:
                    sim_matrix = sim_matrix * self.translate_weight + self.translate_bias
                elif self.post_mul_norm is not None:
                    sim_matrix = self.post_mul_norm(sim_matrix)

        if self.training:
            targets = x.label
            loss_dict = self.loss_fn(sim_matrix, targets, mask, self.modality)
            return loss_dict
        else:
            outputs = {'logit': sim_matrix}

            return outputs

def text_projector(**kwargs):
    """
    Create a text projector.
    :param kwargs:
    :return: projector
    """
    default = dict()
    recursive_update(default, kwargs)
    projector = TextProjector(**default)

    return projector


